{
  # Spark relation config
  spark: {
    app: {
      name: Nebula Exchange 2.0
    }

    master:local

    driver: {
      cores: 1
      maxResultSize: 1G
    }

    executor: {
        memory:1G
    }

    cores:{
      max: 16
    }
  }

  # if the hive is hive-on-spark with derby modeï¼Œ you can ignore this hive configure
  # get the config values from file $HIVE_HOME/conf/hive-site.xml or hive-default.xml

  #  hive: {
  #    warehouse: "hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/"
  #    connectionURL: "jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8"
  #    connectionDriverName: "com.mysql.jdbc.Driver"
  #    connectionUserName: "user"
  #    connectionPassword: "password"
  #  }


  # Nebula Graph relation config
  nebula: {
    address:{
      graph:["127.0.0.1:9669"]
      meta:["127.0.0.1:9559"]
    }
    user: root
    pswd: nebula
    space: test

    # if config graph ssl encrypted transmission
    ssl:{
        # if enable is false, other params of ssl are invalid.
        enable:{
            graph:false
        }
        # ssl sign type: CA or SELF
        sign.type:ca

        # if sign.type is CA, make sure config the ca.param. If you submit exchange application with cluster, please make sure each worker has the ca files.
        ca.param: {
            caCrtFilePath:"/path/caCrtFilePath"
            crtFilePath:"/path/crtFilePath"
            keyFilePath:"/path/keyFilePath"
        }

        # if sign.type is SELF, make sure config the self.param. If you submit exchange application with cluster, please make sure each worker has the ca files.
        self.param: {
            crtFilePath:"/path/crtFilePath"
            keyFilePath:"/path/keyFilePath"
            password:"nebula"
        }
    }


    # parameters for SST import, not required
    path:{
        local:"/tmp"
        remote:"/sst"
        hdfs.namenode: "hdfs://name_node:9000"
    }

    # nebula client connection parameters
    connection {
      # socket connect & execute timeout, unit: millisecond
      timeout: 30000
    }

    error: {
      # max number of failures, if the number of failures is bigger than max, then exit the application.
      max: 32
      # failed import job will be recorded in output path
      output: /tmp/errors
    }

    # use google's RateLimiter to limit the requests send to NebulaGraph
    rate: {
      # the stable throughput of RateLimiter
      limit: 1024
      # Acquires a permit from RateLimiter, unit: MILLISECONDS
      # if it can't be obtained within the specified timeout, then give up the request.
      timeout: 1000
    }
  }

  # Processing tags
  # There are tag config examples for different dataSources.
  tags: [


    # KAFKA
    {
      name: tag-name-7
      type: {
        source: kafka
        sink: client
      }
      service: "kafka.service.address"
      topic: "topic-name"
      groupId:"1"
      # config the field names in kafka value
      fields: [kafka-field-0, kafka-field-1, kafka-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field: [kafka-field-0]
      }
      partition: 10
      batch: 10
      interval.seconds: 10
    }

  ]
}
