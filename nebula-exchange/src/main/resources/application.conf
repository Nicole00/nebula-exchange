{
  # Spark relation config
  spark: {
    app: {
      name: Nebula Exchange 2.0
    }

    master:local

    driver: {
      cores: 1
      maxResultSize: 1G
    }

    executor: {
        memory:1G
    }

    cores:{
      max: 16
    }
  }

  # if the hive is hive-on-spark with derby modeï¼Œ you can ignore this hive configure
  # get the config values from file $HIVE_HOME/conf/hive-site.xml or hive-default.xml

  #  hive: {
  #    warehouse: "hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/"
  #    connectionURL: "jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8"
  #    connectionDriverName: "com.mysql.jdbc.Driver"
  #    connectionUserName: "user"
  #    connectionPassword: "password"
  #  }


  # Nebula Graph relation config
  nebula: {
    address:{
      graph:["192.168.8.170:9669"]
      meta:["192.168.8.170:9559"]
    }
    user: root
    pswd: nebula
    space: test

    # if config graph ssl encrypted transmission
    ssl:{
        # if enable is false, other params of ssl are invalid.
        enable:{
            graph:false
        }
        # ssl sign type: CA or SELF
        sign.type:ca

        # if sign.type is CA, make sure config the ca.param. If you submit exchange application with cluster, please make sure each worker has the ca files.
        ca.param: {
            caCrtFilePath:"/path/caCrtFilePath"
            crtFilePath:"/path/crtFilePath"
            keyFilePath:"/path/keyFilePath"
        }

        # if sign.type is SELF, make sure config the self.param. If you submit exchange application with cluster, please make sure each worker has the ca files.
        self.param: {
            crtFilePath:"/path/crtFilePath"
            keyFilePath:"/path/keyFilePath"
            password:"nebula"
        }
    }


    # parameters for SST import, not required
    path:{
        local:"/tmp"
        remote:"/kafka/sst/edge"
        hdfs.namenode: "hdfs://192.168.8.171:9000"
    }

    # nebula client connection parameters
    connection {
      # socket connect & execute timeout, unit: millisecond
      timeout: 30000
    }

    error: {
      # max number of failures, if the number of failures is bigger than max, then exit the application.
      max: 32
      # failed import job will be recorded in output path
      output: /tmp/errors
    }

    # use google's RateLimiter to limit the requests send to NebulaGraph
    rate: {
      # the stable throughput of RateLimiter
      limit: 1024
      # Acquires a permit from RateLimiter, unit: MILLISECONDS
      # if it can't be obtained within the specified timeout, then give up the request.
      timeout: 1000
    }
  }

  # Processing tags
  # There are tag config examples for different dataSources.
  tags: [
    # KAFKA
    {
      name: person
      type: {
        source: kafka
        sink: sst
      }
      service: "192.168.8.171:9092"
      topics: ["nicole"]
      groupId:"b"
      # config the field names in kafka value
      fields: [age]
      nebula.fields: [likeness]
      vertex: {
        field: name
      }
      partition: 10
      batch: 500
      interval.seconds: 10
    }
  ]

  # Processing tags
  # There are tag config examples for different dataSources.
  edges: [
    # KAFKA
    {
      name: like
      type: {
        source: kafka
        sink: sst
      }
      service: "192.168.8.171:9092"
      topics: ["nicole"]
      groupId:"b"
      # config the field names in kafka value
      fields: [age]
      nebula.fields: [likeness]
      source: {
        field: name
      }
      target:{
        field:name
      }
      ranking:age

      partition: 10
      batch: 500
      interval.seconds: 10
    }
  ]
}
